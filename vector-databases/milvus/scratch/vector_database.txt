References
===========================================================
- https://www.pinecone.io/learn/what-is-similarity-search/
- https://www.pinecone.io/learn/vector-embeddings/
- Article explaining Word2Vec: https://jalammar.github.io/illustrated-word2vec/ 


What is a similarity search?
===========================================================
- Sometimes we need to ask a question like "which objects in our inventory
    are similar to what a user serarched for?".
- This is not possible in a traditional relational database.
- To do this our system or representation of the data needs to be able to
  capture the semantic meaning of the words.

What are vector representations?
===========================================================
- Translate meaning of words in a sentence into a vector space.
- Semantic meaning is measured by the distance between vectors of the objects.
- Word2Vec,GLoVE, USE etc. are popular models for generating embeddings from text
data while CNN models like VGG are often used to create image embeddings.

Finding Nearests Vectors
===========================================================
- K-nearest neighbors is a popular method for finding the neares vectors

Vector-Embeddings
===========================================================
- We can create vector embeddings, which is a list of numbers, to represent
our data.
- Whole paragraphs of text can be reduced to a vector.
- Even numerical data can be reduced to a vector for easier operations.
- Key to Vector Embeddings
    - Vector representation makes it possible to translate semantic similarity 
    as perceived by humans to proximity in a vector space.
- Common approaches to create vector embeddings
    - Neural Networks
    - BERT, Word2Vec, GLoVE.


